---
title: Notes-Deep learning based RecSys utilizing visual content(3)
date: 2019-04-22 13:31:27
tags:
    - RecSys
    - User preference modeling
---

### 参考文献

[1] Wu S, Tang Y, Zhu Y, et al. Session-based Recommendation with Graph Neural Networks[J]. arXiv preprint arXiv:1811.00855, 2018.(AAAI 2019)

## Session-based Recommendation with Graph Neural Networks

#### 介绍

文章指出此前session-based的工作对序列进行建模以估计用户和物品的表示，尽管获得较好的效果，但不能得到准确的用户表示并在一定程度上忽略了浏览物品的变化过程。因此，作者提出了基于图神经网络的SR-GNN模型，将每个session建模成图的形式，用GNN获得各个物品的表示，再通过注意力网络获得每个session的表示，从而该表示融合了全局的偏好和当前session兴趣的信息，最后为每个session预测各物品下次点击的概率。模型架构如图1所示。
![Figure1](https://i.loli.net/2019/04/22/5cbd7bdd1fe8d.png)

#### 方法描述

![Figure2](https://i.loli.net/2019/04/23/5cbeaa1c3676b.png)

##### GNN

模型首先将各序列单独建有向图，再全局相连，由于序列中某个物品可能出现多次，故将边的权值设为边的出现次数除以边起点的出度，最终每个序列得到一个如图2所示n*2n的连接矩阵A。随后，将每个序列一一送入GNN中，获得各个物品的向量表示，GNN更新公式如公式1，2，3，4，5。公式1抽取相邻节点的向量表示，作为GNN的输入；公式2，3对应更新门和重置门的更新方法；公式4根据当前状态，上一时刻状态以及当前更新门状态产生候选状态；公式5结合上一时刻状态和候选状态，在更新门的控制下生成当前时刻状态。
![F1-5](https://i.loli.net/2019/04/23/5cbeae16c84ec.png)

##### 生成序列表示

此前session-based的方法往往假设对应每个序列的用户都存在独立的表示，而SR-GNN中的序列表示直接由序列中的顶点表示产生。序列表示为短期兴趣和长期偏好的结合，其中短期兴趣简单地设为序列中最后一个物品的向量表示，长期偏好通过soft注意力机制产生，最终的序列表示通过短期兴趣和长期偏好表示连接后经过线性变换得到。

##### 生成推荐以及训练方法

得到序列表示后，将序列表示乘以每个候选物品的向量得到得分z，再由softmax计算每个候选物品的概率。训练采用交叉熵作为损失函数，BPTT(Back-Propagation Through Time)更新参数。模型时间复杂度为O(m^2 * n)，其中m为平均序列长度，n为总体序列个数，实际上，m远小于n，因此模型复杂度接近线性。部署时，分为在线离线两个部分，离线计算每个物品的表示，从而在线计算推荐结果，

#### 实验

实验中验证了：引入深度学习的有效性；基于马尔可夫链的模型的独立假设（仅基于上衣状态）不可靠；注意力机制能更好地忽略噪声、表现长期的偏好；图建模的有效性。

##### 不同建图方式的比较

将各个序列聚合成全局图：由于权值的存在，降低了自身序列中高度节点边的重要性，影响了序列整体性，当权值变化时，模型表现下降。
添加高阶节点间的连接：效果变差，并不是所有高阶节点都应该直接相连，中继节点有存在的必要。

##### 不同序列表示的比较

证实了短期兴趣与长期偏好结合的必要；序列中噪声影响较大。
![Figure4](https://i.loli.net/2019/04/23/5cbecd91b7e6f.png)

##### 序列长度分析

实验将数据集根据序列长度阈值5分割为短序列、长序列两个部分。NARM和STAMP模型面向长序列时表现下降，可能的原因分别是忽略了重复动作以及RNN面向长序列的困难。